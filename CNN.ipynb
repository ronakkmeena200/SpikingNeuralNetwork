{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "kilNFZUB048h",
        "outputId": "1a68a25f-1892-4d02-f0cd-c2ed8f21373d"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "#download mnist data and split into train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape,y_train.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#plot the first image in the dataset\n",
        "plt.imshow(X_train[2])\n",
        "np.random.seed(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "JzQ10ryT1KvQ",
        "outputId": "4fea15cd-f08e-49ad-cb0d-b97b7ec761bc"
      },
      "source": [
        "#reshape data to fit model\n",
        "X_train = X_train.reshape(60000,1,28,28)\n",
        "X_test = X_test.reshape(10000,1,28,28)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "#one-hot encode target column\n",
        "#y_train = to_categorical(y_train)\n",
        "#y_test = to_categorical(y_test)\n",
        "plt.imshow(X_train[6,0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdc015f2d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNtxDo0_2rDe"
      },
      "source": [
        "def conv_forward(x, w, b, conv_param):\n",
        "\n",
        "\n",
        "    N, C, H, W = np.shape(x)\n",
        "    F, _, HH, WW = w.shape\n",
        "    stride = conv_param['stride']\n",
        "    pad = conv_param['pad']\n",
        "\n",
        "    H_next = int(1 + (H + 2 * pad - HH) / stride)\n",
        "    W_next = int(1 + (W + 2 * pad - WW) / stride)\n",
        "    out_shape = (N, F, H_next, W_next)\n",
        "    out = np.zeros(out_shape)\n",
        "    \n",
        "    npad = ((0, 0), (0, 0), (1, 1), (1, 1))  # pad around x's 3rd and 4th dimensions\n",
        "    padded_x = np.pad(x, pad_width=npad, mode='constant', constant_values=0)\n",
        "\n",
        "    for n in range(N):\n",
        "      for f in range(F):\n",
        "        for h_stride in range(H_next):\n",
        "          for v_stride in range(W_next):\n",
        "           \n",
        "            region_of_x = padded_x[n, \n",
        "                                   :, # all of the filters/channels\n",
        "                                   h_stride*stride : h_stride*stride+HH,\n",
        "                                   v_stride*stride : v_stride*stride+WW]\n",
        "            out[n, f, h_stride, v_stride] = np.sum(region_of_x * w[f]) + b[f]\n",
        "    cache = (x, w, b, conv_param)\n",
        "    return out, cache        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7L91CtdkZ3v"
      },
      "source": [
        "def conv_backward (dout,cache):\n",
        "  x, w, b, conv_param = cache\n",
        "  N, C, H, W = np.shape(x)\n",
        "  F, _, HH, WW = w.shape\n",
        "  stride = conv_param['stride']\n",
        "  pad = conv_param['pad']\n",
        "  _, _, H_next, W_next = dout.shape\n",
        "\n",
        "  npad = ((0, 0), (0, 0), (1, 1), (1, 1))  # pad around x's 3rd and 4th dimensions\n",
        "  padded_x = np.pad(x, pad_width=npad, mode='constant', constant_values=0)\n",
        "\n",
        "  db = np.sum(dout, axis=(0,2,3))\n",
        "  dw = np.zeros(w.shape)\n",
        "  dpadded_x = np.zeros(padded_x.shape)\n",
        "\n",
        "  for n in range(N):\n",
        "    for f in range(F):\n",
        "      for h_stride in range(H_next):\n",
        "        for v_stride in range(W_next):\n",
        "          dw[f] += padded_x[n, :, \n",
        "                             h_stride*stride : h_stride*stride+HH, \n",
        "                             v_stride*stride : v_stride*stride+WW] * dout[n, f, h_stride, v_stride]\n",
        "          dpadded_x[n, :, h_stride*stride : h_stride*stride+HH, v_stride*stride : v_stride*stride+WW] += w[f] * dout[n, f, h_stride, v_stride]\n",
        "  dx = dpadded_x[:,:,1:-1,1:-1]      \n",
        "    \n",
        "  return dx, dw, db\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajeaRrmSnLhb"
      },
      "source": [
        "def relu_forward(x):\n",
        "  out = np.maximum(0, x)\n",
        "  cache = x\n",
        "  return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "  x = cache\n",
        "  dx = dout\n",
        "  dx[x <= 0] = 0\n",
        "  return dx\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9m4XZ0AvDJW"
      },
      "source": [
        "def affine_forward(x, w, b):\n",
        "  N = np.shape(x)[0]\n",
        "  M = np.product(np.shape(x)[1:])\n",
        "  xReshaped = x.reshape(N, M)\n",
        "  out = np.dot(xReshaped, w) + b\n",
        "\n",
        "  cache = (x, w, b)\n",
        "  return out, cache\n",
        "\n",
        "def affine_backward(dout, cache):\n",
        "\n",
        "  x, w, b = cache\n",
        "\n",
        "  N = np.shape(x)[0]\n",
        "  M = np.product(np.shape(x)[1:])\n",
        "  xReshaped = x.reshape(N, M)\n",
        "  dw = np.dot(xReshaped.T, dout)\n",
        "  dx = np.dot(dout, w.T)                # N,M * M,D\n",
        "  dx = dx.reshape(N, *np.shape(x)[1:])  # reshape to N, d1, d2, d3\n",
        "  db = np.sum(dout, axis=0)\n",
        "  return dx, dw, db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRTSLbF_6w5O"
      },
      "source": [
        "def softmax_loss(x, y):\n",
        "  probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "  probs /= np.sum(probs, axis=1, keepdims=True)\n",
        "  N = x.shape[0]\n",
        "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
        "  dx = probs.copy()\n",
        "  dx[np.arange(N), y] -= 1\n",
        "  dx /= N\n",
        "  return loss,dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bPNggcvB7vh"
      },
      "source": [
        "def loss_func(W1,W2,W3,b1,b2,b3,x,y,conv_param):\n",
        "  #FWD pass\n",
        "  \n",
        "  out1,cache1=conv_forward(x, W1, b1, conv_param) #CONV layer1\n",
        "  \n",
        "  out2,cache2= relu_forward(out1)\n",
        "  #print(x.shape,W1.shape,b1.shape,out1.shape,out2.shape)\n",
        "  out3,cache3=conv_forward(out2, W2, b2, conv_param) #CONV layer2\n",
        "  out4,cache4=  relu_forward(out3)\n",
        "  #print(W2.shape,b2.shape,out3.shape,out4.shape)\n",
        "  scores,cache5=affine_forward(out4, W3, b3)\n",
        "  #print(W3.shape,b3.shape,scores.shape)\n",
        "\n",
        "  loss, dscores = softmax_loss(scores, y)\n",
        "  accuracy=np.sum(np.argmax(scores,axis=1)==y)\n",
        "\n",
        "  #print(loss,dscores.shape)\n",
        "  dout4, dW3, db3 = affine_backward(dscores, cache5)\n",
        " # print(np.unique(cache5[0]))\n",
        "  #print(np.unique(dW3))\n",
        "  dout3=relu_backward(dout4, cache4)\n",
        "  dout2, dW2, db2=conv_backward (dout3,cache3)\n",
        "  dout1=relu_backward(dout2, cache2)\n",
        "  _, dW1, db1=conv_backward (dout1,cache1)\n",
        " \n",
        "  #Updating parameters\n",
        "  grad={'dW1':dW1,'db1':db1,'dW2':dW2,'db2':db2,'dW3':dW3,'db3':db3}\n",
        "  return loss,scores,grad,accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXimqyYShidl"
      },
      "source": [
        "def fwd_pass(W1,W2,W3,b1,b2,b3,x,y,conv_param):\n",
        "  #FWD pass\n",
        "  \n",
        "  out1,_=conv_forward(x, W1, b1, conv_param) #CONV layer1\n",
        "  \n",
        "  out2,_= relu_forward(out1)\n",
        "  #print(x.shape,W1.shape,b1.shape,out1.shape,out2.shape)\n",
        "  out3,_=conv_forward(out2, W2, b2, conv_param) #CONV layer2\n",
        "  out4,_=  relu_forward(out3)\n",
        "  #print(W2.shape,b2.shape,out3.shape,out4.shape)\n",
        "  scores,_=affine_forward(out4, W3, b3)\n",
        "  #print(W3.shape,b3.shape,scores.shape)\n",
        "\n",
        "  loss, dscores = softmax_loss(scores, y)\n",
        "  accuracy=np.sum(np.argmax(scores,axis=1)==y)\n",
        "\n",
        "  return loss,scores,accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XstfotmzGpfG"
      },
      "source": [
        "class AdamOptim():\n",
        "    def __init__(self,num_filters ,num_filters1, C,filter_size, num_classes, hidden_dim,eta=1e-2, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        size1=(num_filters, C, filter_size, filter_size)\n",
        "        size2=(num_filters1, num_filters, filter_size, filter_size)\n",
        "        size3=(hidden_dim, num_classes)\n",
        "\n",
        "        self.m_dw1, self.v_dw1 = np.zeros(size1), np.zeros(size1)\n",
        "        self.m_db1, self.v_db1 = np.zeros(num_filters), np.zeros(num_filters)\n",
        "\n",
        "        self.m_dw2, self.v_dw2 = np.zeros(size2), np.zeros(size2)\n",
        "        self.m_db2, self.v_db2 = np.zeros(num_filters1), np.zeros(num_filters1)\n",
        "\n",
        "        self.m_dw3, self.v_dw3 = np.zeros(size3), np.zeros(size3)\n",
        "        self.m_db3, self.v_db3 = np.zeros(num_classes), np.zeros(num_classes)\n",
        "\n",
        "\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.eta = eta\n",
        "    def update(self,t,w1,w2,w3,b1,b2,b3,dw1,dw2,dw3,db1,db2,db3):\n",
        "        ## dw, db are from current minibatch\n",
        "        ## momentum beta 1\n",
        "        # *** weights *** #\n",
        "        self.m_dw1 = self.beta1*self.m_dw1 + (1-self.beta1)*dw1\n",
        "        self.m_dw2 = self.beta1*self.m_dw2 + (1-self.beta1)*dw2\n",
        "        self.m_dw3 = self.beta1*self.m_dw3 + (1-self.beta1)*dw3\n",
        "        # *** biases *** #\n",
        "        self.m_db1 = self.beta1*self.m_db1 + (1-self.beta1)*db1\n",
        "        self.m_db2 = self.beta1*self.m_db2 + (1-self.beta1)*db2\n",
        "        self.m_db3 = self.beta1*self.m_db3 + (1-self.beta1)*db3\n",
        "\n",
        "        ## rms beta 2\n",
        "        # *** weights *** #\n",
        "        self.v_dw1 = self.beta2*self.v_dw1 + (1-self.beta2)*(dw1**2)\n",
        "        self.v_dw2 = self.beta2*self.v_dw2 + (1-self.beta2)*(dw2**2)\n",
        "        self.v_dw3 = self.beta2*self.v_dw3 + (1-self.beta2)*(dw3**2)\n",
        "        # *** biases *** #\n",
        "        self.v_db1 = self.beta2*self.v_db1 + (1-self.beta2)*(db1**2)\n",
        "        self.v_db2 = self.beta2*self.v_db2 + (1-self.beta2)*(db2**2)\n",
        "        self.v_db3 = self.beta2*self.v_db3 + (1-self.beta2)*(db3**2)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw1_corr = self.m_dw1/(1-self.beta1**t)\n",
        "        m_dw2_corr = self.m_dw2/(1-self.beta1**t)\n",
        "        m_dw3_corr = self.m_dw3/(1-self.beta1**t)\n",
        "\n",
        "        m_db1_corr = self.m_db1/(1-self.beta1**t)\n",
        "        m_db2_corr = self.m_db2/(1-self.beta1**t)\n",
        "        m_db3_corr = self.m_db3/(1-self.beta1**t)\n",
        "\n",
        "        v_dw1_corr = self.v_dw1/(1-self.beta2**t)\n",
        "        v_dw2_corr = self.v_dw2/(1-self.beta2**t)\n",
        "        v_dw3_corr = self.v_dw3/(1-self.beta2**t)\n",
        "\n",
        "        v_db1_corr = self.v_db1/(1-self.beta2**t)\n",
        "        v_db2_corr = self.v_db2/(1-self.beta2**t)\n",
        "        v_db3_corr = self.v_db3/(1-self.beta2**t)\n",
        "\n",
        "        ## update weights and biases\n",
        "        w1 = w1 - self.eta*(m_dw1_corr/(np.sqrt(v_dw1_corr)+self.epsilon))\n",
        "        w2 = w2 - self.eta*(m_dw2_corr/(np.sqrt(v_dw2_corr)+self.epsilon))\n",
        "        w3 = w3 - self.eta*(m_dw3_corr/(np.sqrt(v_dw3_corr)+self.epsilon))\n",
        "        b1 = b1 - self.eta*(m_db1_corr/(np.sqrt(v_db1_corr)+self.epsilon))\n",
        "        b2 = b2 - self.eta*(m_db2_corr/(np.sqrt(v_db2_corr)+self.epsilon))\n",
        "        b3 = b3 - self.eta*(m_db3_corr/(np.sqrt(v_db3_corr)+self.epsilon))\n",
        "        return w1,w2,w3, b1,b2,b3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3Y3O9gAyu_l"
      },
      "source": [
        "\n",
        "num_filters ,num_filters1, C,filter_size, num_classes=8,8,1,3 ,10\n",
        "hidden_dim=num_filters1 * X_train.shape[2] * X_train.shape[3]\n",
        "conv_param = {'stride': 1, 'pad': (filter_size - 1) // 2}\n",
        "weight_scale=1e-2\n",
        "#initializing parameters\n",
        "W1 = np.random.normal(scale=weight_scale,size=(num_filters, C, filter_size, filter_size))\n",
        "b1 = np.zeros(num_filters)\n",
        "W2 = np.random.normal(scale=weight_scale,size=(num_filters1, num_filters, filter_size, filter_size))\n",
        "b2 = np.zeros(num_filters1)\n",
        "W3 = np.random.normal(scale=weight_scale,size=(hidden_dim, num_classes))\n",
        "b3 = np.zeros(num_classes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3w0VsyZ2NN9",
        "outputId": "22e2efc5-c82e-40ea-ff81-1e2f725ca349"
      },
      "source": [
        "\n",
        "batch_size,epoch,t=16,10,3\n",
        "idx=np.arange(X_train.shape[0])\n",
        "np.random.shuffle(idx)\n",
        "N=int(X_train.shape[0]/batch_size)\n",
        "\n",
        "adam = AdamOptim(num_filters ,num_filters1, C,filter_size, num_classes, hidden_dim)\n",
        "l=[]\n",
        "a=[]\n",
        "for i in range(epoch):\n",
        "  print('Epoch no.',i+1)\n",
        "  for j in range(N):\n",
        "    loss,scores,grad,accuracy=loss_func(W1,W2,W3,b1,b2,b3,\n",
        "                               X_train[idx[j*batch_size:(j+1)*batch_size]],\n",
        "                               y_train[idx[j*batch_size:(j+1)*batch_size]],\n",
        "                               conv_param)\n",
        "\n",
        "    W1,W2,W3,b1,b2,b3= adam.update(t,W1,W2,W3,b1,b2,b3,grad['dW1'],grad['dW2'],grad['dW3'],grad['db1'],grad['db2'],grad['db3'] )\n",
        "    l.append(loss)\n",
        "    a.append(accuracy)\n",
        "    \n",
        "    t+=1\n",
        "\n",
        "    if (j%10==0): \n",
        "      print('iteration',j+1,'loss : ',loss ,'Accuracy:',accuracy/batch_size)\n",
        "    if(j==N-1):\n",
        "      print('Avg Accuracy for epoch',i+1,' :',sum(a[i*N:i+1*(N-1)])/N)\n",
        "#out1,cache1=conv_forward(X_train[:100], W1, b1, conv_param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch no. 1\n",
            "iteration 1 loss :  2.2924060077856137 Accuracy: 0.1875\n",
            "iteration 11 loss :  1.9867380628764235 Accuracy: 0.375\n",
            "iteration 21 loss :  0.8257145216750124 Accuracy: 0.8125\n",
            "iteration 31 loss :  0.8049887939485889 Accuracy: 0.75\n",
            "iteration 41 loss :  0.45349341174421764 Accuracy: 0.75\n",
            "iteration 51 loss :  1.3232535983710991 Accuracy: 0.5\n",
            "iteration 61 loss :  0.4548942283743539 Accuracy: 0.875\n",
            "iteration 71 loss :  0.5411731108713921 Accuracy: 0.875\n",
            "iteration 81 loss :  0.49066545613327595 Accuracy: 0.75\n",
            "iteration 91 loss :  0.5499907341558805 Accuracy: 0.875\n",
            "iteration 101 loss :  0.7045329780525675 Accuracy: 0.75\n",
            "iteration 111 loss :  0.6535912429429207 Accuracy: 0.8125\n",
            "iteration 121 loss :  0.9303961742658182 Accuracy: 0.75\n",
            "iteration 131 loss :  0.859767383101913 Accuracy: 0.6875\n",
            "iteration 141 loss :  0.22411893286615409 Accuracy: 0.875\n",
            "iteration 151 loss :  0.6924468543899909 Accuracy: 0.8125\n",
            "iteration 161 loss :  0.2590963038077118 Accuracy: 0.9375\n",
            "iteration 171 loss :  0.6743881033536576 Accuracy: 0.6875\n",
            "iteration 181 loss :  0.19700452386604902 Accuracy: 0.9375\n",
            "iteration 191 loss :  0.2150570139185925 Accuracy: 0.9375\n",
            "iteration 201 loss :  0.04089075367129377 Accuracy: 1.0\n",
            "iteration 211 loss :  0.45099308467357135 Accuracy: 0.8125\n",
            "iteration 221 loss :  0.011561291081010769 Accuracy: 1.0\n",
            "iteration 231 loss :  0.29608891758874767 Accuracy: 0.9375\n",
            "iteration 241 loss :  0.11255032749229682 Accuracy: 1.0\n",
            "iteration 251 loss :  0.6231758241117711 Accuracy: 0.875\n",
            "iteration 261 loss :  0.5043362667079723 Accuracy: 0.8125\n",
            "iteration 271 loss :  0.481596230296767 Accuracy: 0.6875\n",
            "iteration 281 loss :  0.6133586079912261 Accuracy: 0.875\n",
            "iteration 291 loss :  0.5103253238640425 Accuracy: 0.875\n",
            "iteration 301 loss :  0.010133567174997953 Accuracy: 1.0\n",
            "iteration 311 loss :  0.8867128521613465 Accuracy: 0.75\n",
            "iteration 321 loss :  0.43716197377072336 Accuracy: 0.875\n",
            "iteration 331 loss :  0.6968128720595357 Accuracy: 0.75\n",
            "iteration 341 loss :  0.5142359247108199 Accuracy: 0.875\n",
            "iteration 351 loss :  1.1790691253374395 Accuracy: 0.75\n",
            "iteration 361 loss :  0.19642590782822308 Accuracy: 0.9375\n",
            "iteration 371 loss :  0.01037351200425795 Accuracy: 1.0\n",
            "iteration 381 loss :  0.22790527119696416 Accuracy: 0.9375\n",
            "iteration 391 loss :  0.583356009726192 Accuracy: 0.9375\n",
            "iteration 401 loss :  0.5835086482880187 Accuracy: 0.875\n",
            "iteration 411 loss :  0.7205040846491972 Accuracy: 0.8125\n",
            "iteration 421 loss :  0.16712964909917724 Accuracy: 1.0\n",
            "iteration 431 loss :  0.5679773291222598 Accuracy: 0.8125\n",
            "iteration 441 loss :  0.49258981759594256 Accuracy: 0.875\n",
            "iteration 451 loss :  0.02225417604893316 Accuracy: 1.0\n",
            "iteration 461 loss :  0.19609186911485812 Accuracy: 0.9375\n",
            "iteration 471 loss :  0.31704493652114685 Accuracy: 0.8125\n",
            "iteration 481 loss :  0.6069001618816426 Accuracy: 0.875\n",
            "iteration 491 loss :  0.07671216583018287 Accuracy: 0.9375\n",
            "iteration 501 loss :  0.581922743348356 Accuracy: 0.875\n",
            "iteration 511 loss :  0.5833675538708211 Accuracy: 0.9375\n",
            "iteration 521 loss :  0.327232281805851 Accuracy: 0.875\n",
            "iteration 531 loss :  0.501541909182418 Accuracy: 0.875\n",
            "iteration 541 loss :  0.4533613095029999 Accuracy: 0.875\n",
            "iteration 551 loss :  0.2624527176280429 Accuracy: 0.9375\n",
            "iteration 561 loss :  0.7521698811855758 Accuracy: 0.75\n",
            "iteration 571 loss :  0.8302887130552153 Accuracy: 0.75\n",
            "iteration 581 loss :  0.3783802755512458 Accuracy: 0.875\n",
            "iteration 591 loss :  0.07919217700241107 Accuracy: 1.0\n",
            "iteration 601 loss :  0.08943443974385262 Accuracy: 0.9375\n",
            "iteration 611 loss :  0.6444885228276125 Accuracy: 0.8125\n",
            "iteration 621 loss :  0.3265935990278861 Accuracy: 0.9375\n",
            "iteration 631 loss :  0.746642206867613 Accuracy: 0.875\n",
            "iteration 641 loss :  0.5903158920876154 Accuracy: 0.875\n",
            "iteration 651 loss :  0.40921304991757695 Accuracy: 0.875\n",
            "iteration 661 loss :  0.4853393096601638 Accuracy: 0.8125\n",
            "iteration 671 loss :  0.23410644112301823 Accuracy: 0.875\n",
            "iteration 681 loss :  0.08457894823290484 Accuracy: 1.0\n",
            "iteration 691 loss :  0.41787906367562033 Accuracy: 0.875\n",
            "iteration 701 loss :  0.7151410250626985 Accuracy: 0.8125\n",
            "iteration 711 loss :  0.3898572501424642 Accuracy: 0.875\n",
            "iteration 721 loss :  0.12150189892562516 Accuracy: 0.9375\n",
            "iteration 731 loss :  0.15379416524464706 Accuracy: 0.875\n",
            "iteration 741 loss :  0.42861869940807384 Accuracy: 0.875\n",
            "iteration 751 loss :  0.40268912073434465 Accuracy: 0.9375\n",
            "iteration 761 loss :  0.22739211336212073 Accuracy: 0.875\n",
            "iteration 771 loss :  0.99906675451508 Accuracy: 0.75\n",
            "iteration 781 loss :  0.24495327628337454 Accuracy: 0.9375\n",
            "iteration 791 loss :  0.30582950609957493 Accuracy: 0.875\n",
            "iteration 801 loss :  0.10155352461937556 Accuracy: 0.9375\n",
            "iteration 811 loss :  0.15115984028212032 Accuracy: 0.9375\n",
            "iteration 821 loss :  0.18766220767923733 Accuracy: 0.9375\n",
            "iteration 831 loss :  0.05788904744979016 Accuracy: 1.0\n",
            "iteration 841 loss :  0.01600592013079661 Accuracy: 1.0\n",
            "iteration 851 loss :  0.013750349142177452 Accuracy: 1.0\n",
            "iteration 861 loss :  0.4614191718967575 Accuracy: 0.75\n",
            "iteration 871 loss :  0.41321239680472555 Accuracy: 0.875\n",
            "iteration 881 loss :  0.4189082037565596 Accuracy: 0.9375\n",
            "iteration 891 loss :  0.1314851646967412 Accuracy: 0.9375\n",
            "iteration 901 loss :  0.27526648666750664 Accuracy: 0.875\n",
            "iteration 911 loss :  0.18829641356460716 Accuracy: 0.9375\n",
            "iteration 921 loss :  1.139922793894021 Accuracy: 0.625\n",
            "iteration 931 loss :  0.3002793623438037 Accuracy: 0.9375\n",
            "iteration 941 loss :  0.22567292163997954 Accuracy: 0.9375\n",
            "iteration 951 loss :  0.3281989483929666 Accuracy: 0.875\n",
            "iteration 961 loss :  0.3121407577549402 Accuracy: 0.875\n",
            "iteration 971 loss :  0.21954092093412486 Accuracy: 0.875\n",
            "iteration 981 loss :  0.3696001438786481 Accuracy: 0.75\n",
            "iteration 991 loss :  0.45277261447729134 Accuracy: 0.9375\n",
            "iteration 1001 loss :  0.3301502027108008 Accuracy: 0.75\n",
            "iteration 1011 loss :  0.154864523361961 Accuracy: 0.875\n",
            "iteration 1021 loss :  0.07005530372603522 Accuracy: 0.9375\n",
            "iteration 1031 loss :  0.31951515439438083 Accuracy: 0.9375\n",
            "iteration 1041 loss :  0.08704591193363129 Accuracy: 1.0\n",
            "iteration 1051 loss :  0.7267960767763013 Accuracy: 0.75\n",
            "iteration 1061 loss :  0.5496410750665571 Accuracy: 0.875\n",
            "iteration 1071 loss :  0.12989498039130276 Accuracy: 0.9375\n",
            "iteration 1081 loss :  0.34427578581640816 Accuracy: 0.875\n",
            "iteration 1091 loss :  0.22265989557711097 Accuracy: 1.0\n",
            "iteration 1101 loss :  0.6917288348869037 Accuracy: 0.8125\n",
            "iteration 1111 loss :  0.24752864676974298 Accuracy: 0.9375\n",
            "iteration 1121 loss :  0.35995677511339785 Accuracy: 0.8125\n",
            "iteration 1131 loss :  0.07453208783672184 Accuracy: 0.9375\n",
            "iteration 1141 loss :  0.12433757834726819 Accuracy: 0.9375\n",
            "iteration 1151 loss :  0.12520558718227204 Accuracy: 0.9375\n",
            "iteration 1161 loss :  0.043386868080296626 Accuracy: 1.0\n",
            "iteration 1171 loss :  0.02637855642097272 Accuracy: 1.0\n",
            "iteration 1181 loss :  0.4060724459264649 Accuracy: 0.9375\n",
            "iteration 1191 loss :  0.10589505731472311 Accuracy: 0.9375\n",
            "iteration 1201 loss :  0.05040550991694649 Accuracy: 1.0\n",
            "iteration 1211 loss :  0.17124389420938382 Accuracy: 0.9375\n",
            "iteration 1221 loss :  0.016067492822733148 Accuracy: 1.0\n",
            "iteration 1231 loss :  0.2921184027287626 Accuracy: 0.9375\n",
            "iteration 1241 loss :  0.6767982285235616 Accuracy: 0.8125\n",
            "iteration 1251 loss :  0.32313658576453863 Accuracy: 0.875\n",
            "iteration 1261 loss :  0.1309942044011239 Accuracy: 0.9375\n",
            "iteration 1271 loss :  0.20200545058178926 Accuracy: 0.875\n",
            "iteration 1281 loss :  0.5481573169493904 Accuracy: 0.8125\n",
            "iteration 1291 loss :  0.47103462400966745 Accuracy: 0.875\n",
            "iteration 1301 loss :  0.10758138767909847 Accuracy: 1.0\n",
            "iteration 1311 loss :  0.018586178422508086 Accuracy: 1.0\n",
            "iteration 1321 loss :  0.020222682175468104 Accuracy: 1.0\n",
            "iteration 1331 loss :  0.16096241890446503 Accuracy: 0.9375\n",
            "iteration 1341 loss :  0.19616150458592047 Accuracy: 0.9375\n",
            "iteration 1351 loss :  0.041897408392616846 Accuracy: 1.0\n",
            "iteration 1361 loss :  0.28417966621109675 Accuracy: 0.9375\n",
            "iteration 1371 loss :  0.41835308925917636 Accuracy: 0.875\n",
            "iteration 1381 loss :  0.06644363476585687 Accuracy: 1.0\n",
            "iteration 1391 loss :  0.1467736967706929 Accuracy: 0.9375\n",
            "iteration 1401 loss :  0.12114900008500787 Accuracy: 0.9375\n",
            "iteration 1411 loss :  0.11431755649791196 Accuracy: 0.9375\n",
            "iteration 1421 loss :  0.5009268595169064 Accuracy: 0.875\n",
            "iteration 1431 loss :  0.12504468053652135 Accuracy: 1.0\n",
            "iteration 1441 loss :  0.20144170103410705 Accuracy: 0.9375\n",
            "iteration 1451 loss :  0.2584309334275444 Accuracy: 0.875\n",
            "iteration 1461 loss :  0.05002424616796961 Accuracy: 1.0\n",
            "iteration 1471 loss :  0.1928459775834504 Accuracy: 0.875\n",
            "iteration 1481 loss :  0.17256691952747 Accuracy: 0.9375\n",
            "iteration 1491 loss :  0.1337493723778615 Accuracy: 0.9375\n",
            "iteration 1501 loss :  0.3167035627387896 Accuracy: 0.875\n",
            "iteration 1511 loss :  0.4866438558761773 Accuracy: 0.875\n",
            "iteration 1521 loss :  0.5589016930843947 Accuracy: 0.875\n",
            "iteration 1531 loss :  0.07258495396933425 Accuracy: 1.0\n",
            "iteration 1541 loss :  0.2476901834504298 Accuracy: 0.9375\n",
            "iteration 1551 loss :  0.35588138154467036 Accuracy: 0.9375\n",
            "iteration 1561 loss :  0.06413865783488776 Accuracy: 1.0\n",
            "iteration 1571 loss :  0.2091190268990305 Accuracy: 0.875\n",
            "iteration 1581 loss :  0.09254028176659024 Accuracy: 0.9375\n",
            "iteration 1591 loss :  0.5379741319220127 Accuracy: 0.8125\n",
            "iteration 1601 loss :  0.1575627696936237 Accuracy: 0.9375\n",
            "iteration 1611 loss :  0.6321202264778631 Accuracy: 0.75\n",
            "iteration 1621 loss :  0.50473668809262 Accuracy: 0.875\n",
            "iteration 1631 loss :  0.04313917094379693 Accuracy: 1.0\n",
            "iteration 1641 loss :  0.4320890826384654 Accuracy: 0.875\n",
            "iteration 1651 loss :  0.4305499432321353 Accuracy: 0.9375\n",
            "iteration 1661 loss :  0.3093673079906474 Accuracy: 0.875\n",
            "iteration 1671 loss :  0.22585654503769903 Accuracy: 0.875\n",
            "iteration 1681 loss :  1.7401907462247297 Accuracy: 0.625\n",
            "iteration 1691 loss :  0.22106272357204504 Accuracy: 0.9375\n",
            "iteration 1701 loss :  0.07618644051739872 Accuracy: 1.0\n",
            "iteration 1711 loss :  0.2786901195379802 Accuracy: 0.8125\n",
            "iteration 1721 loss :  0.45547020579376285 Accuracy: 0.875\n",
            "iteration 1731 loss :  0.26418828084754475 Accuracy: 0.9375\n",
            "iteration 1741 loss :  0.3568524289493097 Accuracy: 0.9375\n",
            "iteration 1751 loss :  0.04987515864442397 Accuracy: 1.0\n",
            "iteration 1761 loss :  0.03569203515524937 Accuracy: 1.0\n",
            "iteration 1771 loss :  0.015422344306994192 Accuracy: 1.0\n",
            "iteration 1781 loss :  0.26427865021784425 Accuracy: 0.875\n",
            "iteration 1791 loss :  0.497087012235821 Accuracy: 0.8125\n",
            "iteration 1801 loss :  1.1098528744212586 Accuracy: 0.875\n",
            "iteration 1811 loss :  0.08692457013972668 Accuracy: 1.0\n",
            "iteration 1821 loss :  0.3374234989987079 Accuracy: 0.9375\n",
            "iteration 1831 loss :  0.9021340228885104 Accuracy: 0.6875\n",
            "iteration 1841 loss :  0.06009608884829585 Accuracy: 1.0\n",
            "iteration 1851 loss :  0.21131039057901968 Accuracy: 0.9375\n",
            "iteration 1861 loss :  1.086124857841994 Accuracy: 0.8125\n",
            "iteration 1871 loss :  0.7813051500104657 Accuracy: 0.75\n",
            "iteration 1881 loss :  0.04530402217166568 Accuracy: 1.0\n",
            "iteration 1891 loss :  0.8656926324314771 Accuracy: 0.875\n",
            "iteration 1901 loss :  0.6541823905283091 Accuracy: 0.8125\n",
            "iteration 1911 loss :  1.2501862166873439 Accuracy: 0.625\n",
            "iteration 1921 loss :  0.5855512495692466 Accuracy: 0.8125\n",
            "iteration 1931 loss :  0.21460403047401677 Accuracy: 0.875\n",
            "iteration 1941 loss :  0.5798685238581331 Accuracy: 0.8125\n",
            "iteration 1951 loss :  0.25264697413714154 Accuracy: 0.875\n",
            "iteration 1961 loss :  0.020010576448385997 Accuracy: 1.0\n",
            "iteration 1971 loss :  0.4950899805008343 Accuracy: 0.875\n",
            "iteration 1981 loss :  0.1439090357214299 Accuracy: 0.9375\n",
            "iteration 1991 loss :  0.33747000347802086 Accuracy: 0.9375\n",
            "iteration 2001 loss :  0.06999958025299301 Accuracy: 0.9375\n",
            "iteration 2011 loss :  0.272815771923992 Accuracy: 0.875\n",
            "iteration 2021 loss :  0.14555564611605237 Accuracy: 1.0\n",
            "iteration 2031 loss :  0.5905627793303678 Accuracy: 0.9375\n",
            "iteration 2041 loss :  0.5725117345190053 Accuracy: 0.8125\n",
            "iteration 2051 loss :  0.7176243929121815 Accuracy: 0.8125\n",
            "iteration 2061 loss :  0.3946961894772075 Accuracy: 0.8125\n",
            "iteration 2071 loss :  0.4802636777263635 Accuracy: 0.75\n",
            "iteration 2081 loss :  0.12738976692547582 Accuracy: 0.9375\n",
            "iteration 2091 loss :  0.37585781177465005 Accuracy: 0.875\n",
            "iteration 2101 loss :  0.5178574277129363 Accuracy: 0.875\n",
            "iteration 2111 loss :  0.46299178297088195 Accuracy: 0.875\n",
            "iteration 2121 loss :  0.24420897451105478 Accuracy: 0.9375\n",
            "iteration 2131 loss :  0.19049408921752253 Accuracy: 0.9375\n",
            "iteration 2141 loss :  1.0557843860831333 Accuracy: 0.75\n",
            "iteration 2151 loss :  0.2540569895704424 Accuracy: 0.9375\n",
            "iteration 2161 loss :  0.32272530940409977 Accuracy: 0.875\n",
            "iteration 2171 loss :  0.44273305584668565 Accuracy: 0.8125\n",
            "iteration 2181 loss :  0.36071449997072913 Accuracy: 0.875\n",
            "iteration 2191 loss :  0.35956366763457626 Accuracy: 0.875\n",
            "iteration 2201 loss :  0.45833546206436904 Accuracy: 0.875\n",
            "iteration 2211 loss :  0.4961720645171198 Accuracy: 0.8125\n",
            "iteration 2221 loss :  0.37240630596615304 Accuracy: 0.9375\n",
            "iteration 2231 loss :  0.721572461890011 Accuracy: 0.9375\n",
            "iteration 2241 loss :  0.36787438174448084 Accuracy: 0.875\n",
            "iteration 2251 loss :  0.15279594714899702 Accuracy: 1.0\n",
            "iteration 2261 loss :  0.09116759523403127 Accuracy: 1.0\n",
            "iteration 2271 loss :  0.3233584260172298 Accuracy: 0.9375\n",
            "iteration 2281 loss :  0.831606104873458 Accuracy: 0.875\n",
            "iteration 2291 loss :  0.557672970527608 Accuracy: 0.8125\n",
            "iteration 2301 loss :  0.20589569498668633 Accuracy: 0.9375\n",
            "iteration 2311 loss :  0.14637541011502803 Accuracy: 0.9375\n",
            "iteration 2321 loss :  0.2653488912830172 Accuracy: 0.875\n",
            "iteration 2331 loss :  0.013188189817159288 Accuracy: 1.0\n",
            "iteration 2341 loss :  0.1891971859553069 Accuracy: 0.9375\n",
            "iteration 2351 loss :  0.8444233683115502 Accuracy: 0.8125\n",
            "iteration 2361 loss :  0.3800848664888903 Accuracy: 0.875\n",
            "iteration 2371 loss :  0.6778798239589221 Accuracy: 0.875\n",
            "iteration 2381 loss :  0.08421362668641316 Accuracy: 1.0\n",
            "iteration 2391 loss :  0.5740023454899128 Accuracy: 0.8125\n",
            "iteration 2401 loss :  0.06969710194959262 Accuracy: 1.0\n",
            "iteration 2411 loss :  0.48064431138654834 Accuracy: 0.875\n",
            "iteration 2421 loss :  0.0887405768194486 Accuracy: 0.9375\n",
            "iteration 2431 loss :  0.268408249263141 Accuracy: 0.9375\n",
            "iteration 2441 loss :  0.36820675359868893 Accuracy: 0.875\n",
            "iteration 2451 loss :  0.2434084234040662 Accuracy: 0.875\n",
            "iteration 2461 loss :  0.29152682628940885 Accuracy: 0.875\n",
            "iteration 2471 loss :  0.36078440238929843 Accuracy: 0.9375\n",
            "iteration 2481 loss :  0.05187583374568614 Accuracy: 0.9375\n",
            "iteration 2491 loss :  0.5126458979694756 Accuracy: 0.875\n",
            "iteration 2501 loss :  0.2585598534027905 Accuracy: 0.9375\n",
            "iteration 2511 loss :  0.5334178008576896 Accuracy: 0.875\n",
            "iteration 2521 loss :  0.3069182495245259 Accuracy: 0.875\n",
            "iteration 2531 loss :  0.03864471962145917 Accuracy: 1.0\n",
            "iteration 2541 loss :  0.32901801554536064 Accuracy: 0.875\n",
            "iteration 2551 loss :  0.06922523537223102 Accuracy: 1.0\n",
            "iteration 2561 loss :  0.022896040689353424 Accuracy: 1.0\n",
            "iteration 2571 loss :  0.21411668009047946 Accuracy: 0.875\n",
            "iteration 2581 loss :  0.3643202198609466 Accuracy: 0.9375\n",
            "iteration 2591 loss :  0.3903194200153213 Accuracy: 0.8125\n",
            "iteration 2601 loss :  0.5553712414152487 Accuracy: 0.8125\n",
            "iteration 2611 loss :  0.35052980071610934 Accuracy: 0.9375\n",
            "iteration 2621 loss :  0.11916988674152904 Accuracy: 1.0\n",
            "iteration 2631 loss :  0.678131359543765 Accuracy: 0.75\n",
            "iteration 2641 loss :  0.10352008068998979 Accuracy: 0.9375\n",
            "iteration 2651 loss :  0.210172191577196 Accuracy: 0.9375\n",
            "iteration 2661 loss :  0.1984204778028708 Accuracy: 0.875\n",
            "iteration 2671 loss :  0.30821071222506174 Accuracy: 0.9375\n",
            "iteration 2681 loss :  0.3449092407985878 Accuracy: 0.9375\n",
            "iteration 2691 loss :  0.7704832576608173 Accuracy: 0.8125\n",
            "iteration 2701 loss :  0.28604509619223684 Accuracy: 0.875\n",
            "iteration 2711 loss :  0.17960757701930222 Accuracy: 0.9375\n",
            "iteration 2721 loss :  0.09343140995091265 Accuracy: 1.0\n",
            "iteration 2731 loss :  0.17111773326638724 Accuracy: 0.9375\n",
            "iteration 2741 loss :  0.5790465598876294 Accuracy: 0.625\n",
            "iteration 2751 loss :  0.6153340945560843 Accuracy: 0.875\n",
            "iteration 2761 loss :  0.24391006886068012 Accuracy: 0.875\n",
            "iteration 2771 loss :  0.2859502727901217 Accuracy: 0.9375\n",
            "iteration 2781 loss :  0.033069425533985654 Accuracy: 1.0\n",
            "iteration 2791 loss :  0.14801689436731086 Accuracy: 1.0\n",
            "iteration 2801 loss :  0.4119651517662074 Accuracy: 0.875\n",
            "iteration 2811 loss :  0.09965146006399195 Accuracy: 1.0\n",
            "iteration 2821 loss :  0.14940047850590588 Accuracy: 0.9375\n",
            "iteration 2831 loss :  0.05186910461195068 Accuracy: 1.0\n",
            "iteration 2841 loss :  0.12933311875424616 Accuracy: 0.9375\n",
            "iteration 2851 loss :  0.030230655425114085 Accuracy: 1.0\n",
            "iteration 2861 loss :  0.4426578406664789 Accuracy: 0.9375\n",
            "iteration 2871 loss :  0.44866030166365356 Accuracy: 0.875\n",
            "iteration 2881 loss :  0.3576640039542596 Accuracy: 0.9375\n",
            "iteration 2891 loss :  0.6389227991801891 Accuracy: 0.6875\n",
            "iteration 2901 loss :  0.07034965555782036 Accuracy: 1.0\n",
            "iteration 2911 loss :  0.08632708428860335 Accuracy: 1.0\n",
            "iteration 2921 loss :  0.5367979158163172 Accuracy: 0.8125\n",
            "iteration 2931 loss :  0.40627531884197643 Accuracy: 0.875\n",
            "iteration 2941 loss :  0.2400105015375643 Accuracy: 0.9375\n",
            "iteration 2951 loss :  0.14070627241941744 Accuracy: 0.9375\n",
            "iteration 2961 loss :  0.23514941545582813 Accuracy: 0.875\n",
            "iteration 2971 loss :  0.22621809870391224 Accuracy: 0.875\n",
            "iteration 2981 loss :  0.7421488823377422 Accuracy: 0.875\n",
            "iteration 2991 loss :  0.18498385679549273 Accuracy: 0.875\n",
            "iteration 3001 loss :  0.004317221554782563 Accuracy: 1.0\n",
            "iteration 3011 loss :  0.14293173994070618 Accuracy: 0.9375\n",
            "iteration 3021 loss :  0.08787780468155246 Accuracy: 1.0\n",
            "iteration 3031 loss :  0.9869971158417725 Accuracy: 0.875\n",
            "iteration 3041 loss :  0.11807295517263441 Accuracy: 0.9375\n",
            "iteration 3051 loss :  0.4361144288301218 Accuracy: 0.875\n",
            "iteration 3061 loss :  0.03991544812295032 Accuracy: 1.0\n",
            "iteration 3071 loss :  0.3541381961795452 Accuracy: 0.8125\n",
            "iteration 3081 loss :  0.08190757910752536 Accuracy: 1.0\n",
            "iteration 3091 loss :  0.04624057446311262 Accuracy: 1.0\n",
            "iteration 3101 loss :  0.22858080455186247 Accuracy: 0.9375\n",
            "iteration 3111 loss :  0.02020693407992725 Accuracy: 1.0\n",
            "iteration 3121 loss :  0.05427481320625367 Accuracy: 1.0\n",
            "iteration 3131 loss :  0.5423802959651597 Accuracy: 0.8125\n",
            "iteration 3141 loss :  0.6291837501570509 Accuracy: 0.75\n",
            "iteration 3151 loss :  0.39649222592064937 Accuracy: 0.8125\n",
            "iteration 3161 loss :  0.45320503631709824 Accuracy: 0.9375\n",
            "iteration 3171 loss :  0.1281021590912774 Accuracy: 0.9375\n",
            "iteration 3181 loss :  0.027293850639607188 Accuracy: 1.0\n",
            "iteration 3191 loss :  0.19865438346295822 Accuracy: 0.9375\n",
            "iteration 3201 loss :  0.04455534043726467 Accuracy: 1.0\n",
            "iteration 3211 loss :  0.12730074421974721 Accuracy: 0.875\n",
            "iteration 3221 loss :  0.1432734348479751 Accuracy: 0.9375\n",
            "iteration 3231 loss :  0.018910390827401306 Accuracy: 1.0\n",
            "iteration 3241 loss :  0.014664169475718527 Accuracy: 1.0\n",
            "iteration 3251 loss :  0.17156890650470125 Accuracy: 0.875\n",
            "iteration 3261 loss :  0.0958785545749495 Accuracy: 0.9375\n",
            "iteration 3271 loss :  0.46627923074858446 Accuracy: 0.875\n",
            "iteration 3281 loss :  0.1677452235257213 Accuracy: 0.9375\n",
            "iteration 3291 loss :  0.032424477549625165 Accuracy: 1.0\n",
            "iteration 3301 loss :  0.1422503013084901 Accuracy: 1.0\n",
            "iteration 3311 loss :  0.29434092406828116 Accuracy: 0.875\n",
            "iteration 3321 loss :  1.15029872114851 Accuracy: 0.625\n",
            "iteration 3331 loss :  0.28380982102330904 Accuracy: 0.8125\n",
            "iteration 3341 loss :  0.17115169246138362 Accuracy: 0.9375\n",
            "iteration 3351 loss :  0.16803607715250615 Accuracy: 0.9375\n",
            "iteration 3361 loss :  0.11152935870613881 Accuracy: 1.0\n",
            "iteration 3371 loss :  0.1649496963376874 Accuracy: 0.9375\n",
            "iteration 3381 loss :  0.7524286051698137 Accuracy: 0.875\n",
            "iteration 3391 loss :  0.057538142504502 Accuracy: 0.9375\n",
            "iteration 3401 loss :  0.05950621515449846 Accuracy: 1.0\n",
            "iteration 3411 loss :  0.07482937773804868 Accuracy: 0.9375\n",
            "iteration 3421 loss :  0.06639372782276041 Accuracy: 0.9375\n",
            "iteration 3431 loss :  0.05409155185307624 Accuracy: 1.0\n",
            "iteration 3441 loss :  0.08749217595236616 Accuracy: 0.9375\n",
            "iteration 3451 loss :  0.10756879395684896 Accuracy: 0.9375\n",
            "iteration 3461 loss :  0.12577406025405855 Accuracy: 0.9375\n",
            "iteration 3471 loss :  0.5702886077345387 Accuracy: 0.8125\n",
            "iteration 3481 loss :  0.16049824829878853 Accuracy: 0.9375\n",
            "iteration 3491 loss :  0.3461086757050806 Accuracy: 0.875\n",
            "iteration 3501 loss :  0.12037662329525861 Accuracy: 0.9375\n",
            "iteration 3511 loss :  0.04044806078223248 Accuracy: 1.0\n",
            "iteration 3521 loss :  0.03544689445339017 Accuracy: 1.0\n",
            "iteration 3531 loss :  0.061572084045947495 Accuracy: 1.0\n",
            "iteration 3541 loss :  0.18723964480669436 Accuracy: 0.9375\n",
            "iteration 3551 loss :  0.4935614162319644 Accuracy: 0.875\n",
            "iteration 3561 loss :  0.15164357994082298 Accuracy: 0.9375\n",
            "iteration 3571 loss :  0.0586549688531021 Accuracy: 0.9375\n",
            "iteration 3581 loss :  0.2135272630763526 Accuracy: 0.875\n",
            "iteration 3591 loss :  0.08417116736459695 Accuracy: 1.0\n",
            "iteration 3601 loss :  0.8554536704263697 Accuracy: 0.875\n",
            "iteration 3611 loss :  0.25769315697254896 Accuracy: 0.9375\n",
            "iteration 3621 loss :  0.12777854667995775 Accuracy: 1.0\n",
            "iteration 3631 loss :  0.031712323728516685 Accuracy: 1.0\n",
            "iteration 3641 loss :  0.016787283428876306 Accuracy: 1.0\n",
            "iteration 3651 loss :  0.38640166706011975 Accuracy: 0.875\n",
            "iteration 3661 loss :  0.05966900231191338 Accuracy: 1.0\n",
            "iteration 3671 loss :  0.3321159096151543 Accuracy: 0.875\n",
            "iteration 3681 loss :  0.5162692930516395 Accuracy: 0.8125\n",
            "iteration 3691 loss :  0.11386242073054678 Accuracy: 0.9375\n",
            "iteration 3701 loss :  0.10277911514728111 Accuracy: 0.9375\n",
            "iteration 3711 loss :  0.039166079608092305 Accuracy: 1.0\n",
            "iteration 3721 loss :  0.23594121592365017 Accuracy: 0.9375\n",
            "iteration 3731 loss :  0.22921974601430878 Accuracy: 0.9375\n",
            "iteration 3741 loss :  0.07835102129739109 Accuracy: 1.0\n",
            "Avg Accuracy for epoch 1  : 14.3992\n",
            "Epoch no. 2\n",
            "iteration 1 loss :  0.5597057454353959 Accuracy: 0.8125\n",
            "iteration 11 loss :  0.3170214094555338 Accuracy: 0.875\n",
            "iteration 21 loss :  0.02001583785051976 Accuracy: 1.0\n",
            "iteration 31 loss :  0.015973725799199687 Accuracy: 1.0\n",
            "iteration 41 loss :  0.04225185292401978 Accuracy: 1.0\n",
            "iteration 51 loss :  0.7650580642956617 Accuracy: 0.8125\n",
            "iteration 61 loss :  0.6001340037537024 Accuracy: 0.8125\n",
            "iteration 71 loss :  0.35014937944621 Accuracy: 0.875\n",
            "iteration 81 loss :  0.1834184482470711 Accuracy: 0.9375\n",
            "iteration 91 loss :  0.3467725251821152 Accuracy: 0.875\n",
            "iteration 101 loss :  0.31891295816448006 Accuracy: 0.875\n",
            "iteration 111 loss :  0.544586269907427 Accuracy: 0.875\n",
            "iteration 121 loss :  0.8247729385562029 Accuracy: 0.8125\n",
            "iteration 131 loss :  0.667170785027258 Accuracy: 0.9375\n",
            "iteration 141 loss :  0.4420683250853106 Accuracy: 0.8125\n",
            "iteration 151 loss :  0.07925810620893178 Accuracy: 0.9375\n",
            "iteration 161 loss :  0.46649162376899733 Accuracy: 0.9375\n",
            "iteration 171 loss :  0.25508734884133577 Accuracy: 0.9375\n",
            "iteration 181 loss :  0.024315536934339764 Accuracy: 1.0\n",
            "iteration 191 loss :  0.21207450041836368 Accuracy: 0.9375\n",
            "iteration 201 loss :  0.05202317367934494 Accuracy: 1.0\n",
            "iteration 211 loss :  0.15490453402701232 Accuracy: 0.875\n",
            "iteration 221 loss :  0.027338683421459856 Accuracy: 1.0\n",
            "iteration 231 loss :  0.2714263303390805 Accuracy: 0.9375\n",
            "iteration 241 loss :  0.023959160020184618 Accuracy: 1.0\n",
            "iteration 251 loss :  0.8747870117304992 Accuracy: 0.8125\n",
            "iteration 261 loss :  0.25149945870347923 Accuracy: 0.9375\n",
            "iteration 271 loss :  0.33764841288981917 Accuracy: 0.9375\n",
            "iteration 281 loss :  0.29263556446250155 Accuracy: 0.875\n",
            "iteration 291 loss :  0.12042777468378968 Accuracy: 1.0\n",
            "iteration 301 loss :  0.2817118896221481 Accuracy: 0.875\n",
            "iteration 311 loss :  0.6588930412805508 Accuracy: 0.875\n",
            "iteration 321 loss :  0.19616588764034143 Accuracy: 0.875\n",
            "iteration 331 loss :  0.2933985001688351 Accuracy: 0.9375\n",
            "iteration 341 loss :  0.11534324896167987 Accuracy: 1.0\n",
            "iteration 351 loss :  1.0455503618230098 Accuracy: 0.75\n",
            "iteration 361 loss :  0.08917784966698263 Accuracy: 0.9375\n",
            "iteration 371 loss :  0.17623233961647183 Accuracy: 0.9375\n",
            "iteration 381 loss :  0.14977763902146238 Accuracy: 0.9375\n",
            "iteration 391 loss :  0.49977162670630687 Accuracy: 0.875\n",
            "iteration 401 loss :  0.14705397290160355 Accuracy: 0.9375\n",
            "iteration 411 loss :  0.0685307115773201 Accuracy: 0.9375\n",
            "iteration 421 loss :  0.10843774507316734 Accuracy: 0.9375\n",
            "iteration 431 loss :  0.19561387104671246 Accuracy: 0.875\n",
            "iteration 441 loss :  0.4882985777071967 Accuracy: 0.875\n",
            "iteration 451 loss :  0.03148807809499038 Accuracy: 1.0\n",
            "iteration 461 loss :  0.2274988810579805 Accuracy: 0.875\n",
            "iteration 471 loss :  0.653419081790798 Accuracy: 0.8125\n",
            "iteration 481 loss :  0.658002672939534 Accuracy: 0.8125\n",
            "iteration 491 loss :  0.058402166061351526 Accuracy: 1.0\n",
            "iteration 501 loss :  0.5138697332000416 Accuracy: 0.875\n",
            "iteration 511 loss :  0.2675154003485555 Accuracy: 0.875\n",
            "iteration 521 loss :  0.47825301556139344 Accuracy: 0.75\n",
            "iteration 531 loss :  0.3564219139764796 Accuracy: 0.875\n",
            "iteration 541 loss :  0.6891758128201884 Accuracy: 0.8125\n",
            "iteration 551 loss :  0.1397907573703199 Accuracy: 0.9375\n",
            "iteration 561 loss :  0.29495274783326486 Accuracy: 0.9375\n",
            "iteration 571 loss :  0.626399587387365 Accuracy: 0.6875\n",
            "iteration 581 loss :  0.386485949581814 Accuracy: 0.875\n",
            "iteration 591 loss :  0.25207784739218436 Accuracy: 0.9375\n",
            "iteration 601 loss :  0.03337479246677673 Accuracy: 1.0\n",
            "iteration 611 loss :  0.6714722264249724 Accuracy: 0.875\n",
            "iteration 621 loss :  0.38593433692005946 Accuracy: 0.875\n",
            "iteration 631 loss :  0.3969219142387323 Accuracy: 0.875\n",
            "iteration 641 loss :  0.3402164272102691 Accuracy: 0.875\n",
            "iteration 651 loss :  0.21594638142832465 Accuracy: 0.8125\n",
            "iteration 661 loss :  0.06775823158769731 Accuracy: 1.0\n",
            "iteration 671 loss :  0.19089046976622018 Accuracy: 0.9375\n",
            "iteration 681 loss :  0.053645574986333974 Accuracy: 1.0\n",
            "iteration 691 loss :  0.37392990595975195 Accuracy: 0.875\n",
            "iteration 701 loss :  0.48852084528036055 Accuracy: 0.875\n",
            "iteration 711 loss :  0.25648720589459373 Accuracy: 0.9375\n",
            "iteration 721 loss :  0.12379333592525138 Accuracy: 1.0\n",
            "iteration 731 loss :  0.12391140785872362 Accuracy: 1.0\n",
            "iteration 741 loss :  0.12643211732128928 Accuracy: 0.9375\n",
            "iteration 751 loss :  0.5291291863243921 Accuracy: 0.875\n",
            "iteration 761 loss :  0.12890760662156608 Accuracy: 0.9375\n",
            "iteration 771 loss :  0.7850439366778339 Accuracy: 0.875\n",
            "iteration 781 loss :  0.16257569573212277 Accuracy: 0.9375\n",
            "iteration 791 loss :  0.006932578537139453 Accuracy: 1.0\n",
            "iteration 801 loss :  0.297196272800822 Accuracy: 0.9375\n",
            "iteration 811 loss :  0.15420215593465825 Accuracy: 0.9375\n",
            "iteration 821 loss :  0.13953022618615432 Accuracy: 0.9375\n",
            "iteration 831 loss :  0.16797457740825394 Accuracy: 0.875\n",
            "iteration 841 loss :  0.05827297516264958 Accuracy: 1.0\n",
            "iteration 851 loss :  0.05021496074405519 Accuracy: 1.0\n",
            "iteration 861 loss :  0.9260769737445378 Accuracy: 0.6875\n",
            "iteration 871 loss :  0.2148101230814842 Accuracy: 0.875\n",
            "iteration 881 loss :  0.048531429198860145 Accuracy: 1.0\n",
            "iteration 891 loss :  0.04547445965399583 Accuracy: 1.0\n",
            "iteration 901 loss :  0.11857964749305676 Accuracy: 0.9375\n",
            "iteration 911 loss :  0.21490156230444615 Accuracy: 0.875\n",
            "iteration 921 loss :  0.22164295655810873 Accuracy: 0.9375\n",
            "iteration 931 loss :  0.31070267321269895 Accuracy: 0.875\n",
            "iteration 941 loss :  0.10837309581182378 Accuracy: 0.9375\n",
            "iteration 951 loss :  0.2622976262464487 Accuracy: 0.875\n",
            "iteration 961 loss :  0.03498341885953801 Accuracy: 1.0\n",
            "iteration 971 loss :  0.16974438534891184 Accuracy: 0.9375\n",
            "iteration 981 loss :  0.14331038998271084 Accuracy: 0.9375\n",
            "iteration 991 loss :  0.17888853498214186 Accuracy: 0.9375\n",
            "iteration 1001 loss :  0.13526525492061006 Accuracy: 1.0\n",
            "iteration 1011 loss :  0.22215831959683002 Accuracy: 0.9375\n",
            "iteration 1021 loss :  0.2942927166365849 Accuracy: 0.8125\n",
            "iteration 1031 loss :  0.17927214340886932 Accuracy: 0.9375\n",
            "iteration 1041 loss :  0.08766436511118536 Accuracy: 0.9375\n",
            "iteration 1051 loss :  0.7172989724735486 Accuracy: 0.8125\n",
            "iteration 1061 loss :  0.235997335685577 Accuracy: 0.9375\n",
            "iteration 1071 loss :  0.39411216871158705 Accuracy: 0.75\n",
            "iteration 1081 loss :  0.3657360827139237 Accuracy: 0.8125\n",
            "iteration 1091 loss :  0.1400404061619614 Accuracy: 0.9375\n",
            "iteration 1101 loss :  0.6164669869957831 Accuracy: 0.8125\n",
            "iteration 1111 loss :  0.2541263832465772 Accuracy: 0.875\n",
            "iteration 1121 loss :  0.7519154159164188 Accuracy: 0.6875\n",
            "iteration 1131 loss :  0.17383883522678595 Accuracy: 0.875\n",
            "iteration 1141 loss :  0.04743127175628372 Accuracy: 1.0\n",
            "iteration 1151 loss :  0.027990102020265036 Accuracy: 1.0\n",
            "iteration 1161 loss :  0.10945410124697644 Accuracy: 1.0\n",
            "iteration 1171 loss :  0.11528240112946074 Accuracy: 0.9375\n",
            "iteration 1181 loss :  0.19912632429291727 Accuracy: 1.0\n",
            "iteration 1191 loss :  0.19058276220334208 Accuracy: 0.9375\n",
            "iteration 1201 loss :  0.010034688686499557 Accuracy: 1.0\n",
            "iteration 1211 loss :  0.03790552378628215 Accuracy: 1.0\n",
            "iteration 1221 loss :  0.034271204132568754 Accuracy: 1.0\n",
            "iteration 1231 loss :  0.3839914624832708 Accuracy: 0.9375\n",
            "iteration 1241 loss :  0.07402596355830993 Accuracy: 1.0\n",
            "iteration 1251 loss :  0.48210348574995954 Accuracy: 0.875\n",
            "iteration 1261 loss :  0.5650782540634951 Accuracy: 0.8125\n",
            "iteration 1271 loss :  0.8157504827807902 Accuracy: 0.5625\n",
            "iteration 1281 loss :  0.6555075174186151 Accuracy: 0.875\n",
            "iteration 1291 loss :  0.3425264843588488 Accuracy: 0.875\n",
            "iteration 1301 loss :  0.04996554624363632 Accuracy: 1.0\n",
            "iteration 1311 loss :  0.06930601061563749 Accuracy: 1.0\n",
            "iteration 1321 loss :  0.08032993917303909 Accuracy: 0.9375\n",
            "iteration 1331 loss :  0.34986623362719926 Accuracy: 0.9375\n",
            "iteration 1341 loss :  0.6688083287507892 Accuracy: 0.8125\n",
            "iteration 1351 loss :  0.187338167385702 Accuracy: 0.9375\n",
            "iteration 1361 loss :  0.31373599582900763 Accuracy: 0.8125\n",
            "iteration 1371 loss :  0.35606485630509727 Accuracy: 0.875\n",
            "iteration 1381 loss :  0.2505587786385064 Accuracy: 0.9375\n",
            "iteration 1391 loss :  0.2271537651255341 Accuracy: 0.875\n",
            "iteration 1401 loss :  0.10920517950753991 Accuracy: 1.0\n",
            "iteration 1411 loss :  0.2916207605377662 Accuracy: 0.9375\n",
            "iteration 1421 loss :  0.23183687683091486 Accuracy: 0.9375\n",
            "iteration 1431 loss :  0.07460959449148931 Accuracy: 1.0\n",
            "iteration 1441 loss :  0.4799260012865875 Accuracy: 0.875\n",
            "iteration 1451 loss :  0.07817462936351852 Accuracy: 0.9375\n",
            "iteration 1461 loss :  0.05790610365797054 Accuracy: 1.0\n",
            "iteration 1471 loss :  0.5753878168350173 Accuracy: 0.875\n",
            "iteration 1481 loss :  0.24810504881247716 Accuracy: 0.875\n",
            "iteration 1491 loss :  0.052978301576131454 Accuracy: 1.0\n",
            "iteration 1501 loss :  0.07617636147295971 Accuracy: 0.9375\n",
            "iteration 1511 loss :  0.7477348913223314 Accuracy: 0.8125\n",
            "iteration 1521 loss :  0.40422681705545815 Accuracy: 0.9375\n",
            "iteration 1531 loss :  0.04560599393423319 Accuracy: 1.0\n",
            "iteration 1541 loss :  0.2586084116602353 Accuracy: 0.875\n",
            "iteration 1551 loss :  0.36249935849879916 Accuracy: 0.8125\n",
            "iteration 1561 loss :  0.09074018975225177 Accuracy: 1.0\n",
            "iteration 1571 loss :  0.24610791880399172 Accuracy: 0.9375\n",
            "iteration 1581 loss :  0.08250943841590214 Accuracy: 0.9375\n",
            "iteration 1591 loss :  0.5096870872536321 Accuracy: 0.8125\n",
            "iteration 1601 loss :  0.07405569075820856 Accuracy: 1.0\n",
            "iteration 1611 loss :  0.3077962052024321 Accuracy: 0.8125\n",
            "iteration 1621 loss :  0.07521542288055048 Accuracy: 1.0\n",
            "iteration 1631 loss :  0.07244975901189979 Accuracy: 0.9375\n",
            "iteration 1641 loss :  0.43039230632859 Accuracy: 0.875\n",
            "iteration 1651 loss :  0.2185187727134047 Accuracy: 0.8125\n",
            "iteration 1661 loss :  0.41814004130022503 Accuracy: 0.875\n",
            "iteration 1671 loss :  0.1844885950501966 Accuracy: 0.9375\n",
            "iteration 1681 loss :  1.389383736862408 Accuracy: 0.75\n",
            "iteration 1691 loss :  0.09653037702185438 Accuracy: 0.9375\n",
            "iteration 1701 loss :  0.01371064277717144 Accuracy: 1.0\n",
            "iteration 1711 loss :  0.16721233776224845 Accuracy: 0.9375\n",
            "iteration 1721 loss :  0.17532697805087996 Accuracy: 0.9375\n",
            "iteration 1731 loss :  0.06520540119461701 Accuracy: 1.0\n",
            "iteration 1741 loss :  0.08208756314592722 Accuracy: 1.0\n",
            "iteration 1751 loss :  0.020814090924432427 Accuracy: 1.0\n",
            "iteration 1761 loss :  0.024860253650158735 Accuracy: 1.0\n",
            "iteration 1771 loss :  0.016527118758391413 Accuracy: 1.0\n",
            "iteration 1781 loss :  0.44970699486375837 Accuracy: 0.875\n",
            "iteration 1791 loss :  0.18633369742385086 Accuracy: 0.9375\n",
            "iteration 1801 loss :  1.3675260104637474 Accuracy: 0.8125\n",
            "iteration 1811 loss :  0.043522626108751315 Accuracy: 1.0\n",
            "iteration 1821 loss :  0.28579915585479 Accuracy: 0.9375\n",
            "iteration 1831 loss :  0.7952132730074454 Accuracy: 0.6875\n",
            "iteration 1841 loss :  0.054737180491276016 Accuracy: 1.0\n",
            "iteration 1851 loss :  0.19202635416036545 Accuracy: 0.9375\n",
            "iteration 1861 loss :  0.4126738875194777 Accuracy: 0.9375\n",
            "iteration 1871 loss :  0.5750412549393853 Accuracy: 0.9375\n",
            "iteration 1881 loss :  0.04558303773782633 Accuracy: 1.0\n",
            "iteration 1891 loss :  0.42661274145211586 Accuracy: 0.875\n",
            "iteration 1901 loss :  0.32843438468592756 Accuracy: 0.875\n",
            "iteration 1911 loss :  0.5495652540268458 Accuracy: 0.9375\n",
            "iteration 1921 loss :  0.03283437788847608 Accuracy: 1.0\n",
            "iteration 1931 loss :  0.10126414401433717 Accuracy: 0.9375\n",
            "iteration 1941 loss :  0.5141982897389635 Accuracy: 0.875\n",
            "iteration 1951 loss :  0.20300696416632374 Accuracy: 0.875\n",
            "iteration 1961 loss :  0.03104395300576192 Accuracy: 1.0\n",
            "iteration 1971 loss :  0.2345580478835696 Accuracy: 0.9375\n",
            "iteration 1981 loss :  0.10236646483859631 Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7voUHJx9rxB"
      },
      "source": [
        "#print(out1.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ncFKoPR9tlN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}